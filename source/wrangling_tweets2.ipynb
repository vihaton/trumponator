{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = '../data/raw/Raw_tweets_DonaldTrump_before_elec.json'\n",
    "after = '../data/raw/Raw_tweets_DonaldTrump_after_elec.json'\n",
    "\n",
    "df_before = pd.read_json(open(before, 'r'))\n",
    "df_after = pd.read_json(open(after, 'r'))\n",
    "\n",
    "data = [df_before, df_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13132"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual method for cleaning the text\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, 'https', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "\n",
    "    # Do we want to do something for links etc?\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", stripped)\n",
    "    lower_case = letters_only.lower()\n",
    "\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the cleaned dataframe\n",
    "def make_clean_df(df):\n",
    "    print(\"Cleaning and parsing the tweets...\\n\")\n",
    "    clean_texts = []\n",
    "    for i in range(0, len(df)):\n",
    "        if( (i+1)%1000 == 0 ):\n",
    "            print(i+1, \" tweets has been processed\")                                                                  \n",
    "        clean_texts.append(tweet_cleaner(df['text'][i]))\n",
    "    clean_df = pd.DataFrame(clean_texts,columns=['text'])\n",
    "    clean_df['timestamp'] = df.created_at\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "1000  tweets has been processed\n",
      "2000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/ZQ0osiFEJQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/SmTkLPiBYD\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/T5JBFXOz3F\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000  tweets has been processed\n",
      "6000  tweets has been processed\n",
      "7000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://t.co/PtViAyrO4A\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000  tweets has been processed\n",
      "9000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/h43dehf0WV\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/FXqSWusSTV\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/qZSEifBNaP\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/MgkotGmkJ0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/hIw1AQdRpY\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/LdXQb42Imc\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/RzqoiQ4SmD\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/zMg2iZgriM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/G0BjCXEnaX\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/Vu2b2hhwHu\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/54YVC4DDfe\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/Vh47XjGzpt\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/c79zLeREOA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/H2FiSVxyOF\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/0a25gApyJ6\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/6ZG0P6FRs5\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/6v90Th0zl1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/3PAVDdfJJr\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/15ibBbf34U\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/wYCNmkkaNR\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/mJtO0AFLus\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/OGqKufBeHn\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/PeF12D2IqJ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/TfRmZA8RWQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/5kIR5EggBp\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/TmICRUV9uo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/Gm9KE8cHpS\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/E3xvdUGZqa\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/ue5JEZy85v\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/yfwdyUHmn3\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/tJG3KIn2q0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/4OjDqTMEIx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/8lI2lomGkh\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/6VLQYAlcto\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/UM3YJ6lUiD\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000  tweets has been processed\n",
      "13000  tweets has been processed\n",
      "Cleaning and parsing the tweets...\n",
      "\n",
      "1000  tweets has been processed\n",
      "2000  tweets has been processed\n",
      "3000  tweets has been processed\n",
      "4000  tweets has been processed\n",
      "5000  tweets has been processed\n"
     ]
    }
   ],
   "source": [
    "clean_df_before = make_clean_df(df_before)\n",
    "clean_df_after = make_clean_df(df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13132"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_df_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5341"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "0      thank you iowa get out votetrumppence https https   \n",
      "1                 rt https thank you new hampshire https   \n",
      "2      van jones there is a crack in the blue wall it...   \n",
      "3      great night in denver colorado thank you toget...   \n",
      "4      rt https join https live in denver colorado vi...   \n",
      "5      thank you reno nevada nothing will stop us in ...   \n",
      "6                join me live in reno nevada https https   \n",
      "7      join me tomorrow minnesota pm https michigan p...   \n",
      "8                                    draintheswamp https   \n",
      "9      top clinton aides bemoan campaign all tactics ...   \n",
      "10     must act immediately clinton charity lawyer to...   \n",
      "11     watch coach mike ditka a great guy and support...   \n",
      "12     thank you wilmington north carolina we are day...   \n",
      "13     thank you for the incredible support this morn...   \n",
      "14     join me in denver colorado tonight at pm https...   \n",
      "15                              make america great again   \n",
      "16     thank you hershey pennsylvania get out vote on...   \n",
      "17     join me in denver colorado tomorrow at pm tick...   \n",
      "18     join me live in hershey pennsylvania makeameri...   \n",
      "19     the only thing that can stop this corrupt mach...   \n",
      "20     thank you ohio vote so we can replace obamacar...   \n",
      "21                 join me live in wilmington ohio https   \n",
      "22     if obama worked as hard on straightening out o...   \n",
      "23     ice officers warn hillary immigration plan wil...   \n",
      "24     thank you nh we will end illegal immigration s...   \n",
      "25     clinton aides definitely not releasing some hr...   \n",
      "26     rt https mrs saucier s son is in prison for ha...   \n",
      "27     the clinton campaign at obama justice drainthe...   \n",
      "28     join me today in wilmington ohio at pm https t...   \n",
      "29     there is no challenge too great no dream outsi...   \n",
      "...                                                  ...   \n",
      "18443  rt https fnc dominated ratings last night msnb...   \n",
      "18444  rt https thank you https for all the help you ...   \n",
      "18445  it is a miracle how fast the las vegas metropo...   \n",
      "18446  so proud of https military and first responder...   \n",
      "18447  great meeting with governor mapp of the usvi h...   \n",
      "18448  leaving puerto rico now for d c will be in las...   \n",
      "18449  i am so proud of our great country god bless a...   \n",
      "18450  my warmest condolences and sympathies to the v...   \n",
      "18451  congratulations to teamusa on your great https...   \n",
      "18452  being nice to rocket man hasn t worked in year...   \n",
      "18453  save your energy rex we ll do what has to be done   \n",
      "18454  i told rex tillerson our wonderful secretary o...   \n",
      "18455  for safety thank you to the governor of p r an...   \n",
      "18456  people are now starting to recognize the amazi...   \n",
      "18457  we have done a great job with the almost impos...   \n",
      "18458  respecting our national anthem standforouranth...   \n",
      "18459  in analyzing the alabama primary race fake new...   \n",
      "18460  because of fakenews my people are not getting ...   \n",
      "18461  very important that nfl players stand tomorrow...   \n",
      "18462  i will be in pr on tues to further ensure we c...   \n",
      "18463  results of recovery efforts will speak much lo...   \n",
      "18464  we must all be united in offering assistance t...   \n",
      "18465  https fakenews critics are working overtime bu...   \n",
      "18466  my administration governor https and many othe...   \n",
      "18467  to the people of puerto rico do not believe th...   \n",
      "18468  congresswoman jennifer gonzalez colon of puert...   \n",
      "18469  just spoke to governor kenneth mapp of the u s...   \n",
      "18470  the governor of puerto rico ricardo rossello i...   \n",
      "18471  despite the fake news media in conjunction wit...   \n",
      "18472  the fake news networks are working overtime in...   \n",
      "\n",
      "                                        timestamp  \n",
      "0      datetime.datetime(2016, 11, 6, 19, 26, 58)  \n",
      "1       datetime.datetime(2016, 11, 6, 18, 9, 44)  \n",
      "2        datetime.datetime(2016, 11, 6, 17, 6, 8)  \n",
      "3        datetime.datetime(2016, 11, 6, 6, 8, 32)  \n",
      "4           datetime.datetime(2016, 11, 6, 4, 27)  \n",
      "5        datetime.datetime(2016, 11, 6, 2, 3, 20)  \n",
      "6       datetime.datetime(2016, 11, 6, 0, 20, 16)  \n",
      "7       datetime.datetime(2016, 11, 6, 0, 19, 18)  \n",
      "8      datetime.datetime(2016, 11, 5, 21, 46, 34)  \n",
      "9      datetime.datetime(2016, 11, 5, 21, 39, 44)  \n",
      "10      datetime.datetime(2016, 11, 5, 21, 38, 1)  \n",
      "11     datetime.datetime(2016, 11, 5, 21, 27, 32)  \n",
      "12     datetime.datetime(2016, 11, 5, 20, 13, 34)  \n",
      "13      datetime.datetime(2016, 11, 5, 20, 6, 21)  \n",
      "14      datetime.datetime(2016, 11, 5, 16, 45, 5)  \n",
      "15     datetime.datetime(2016, 11, 5, 13, 33, 10)  \n",
      "16      datetime.datetime(2016, 11, 5, 2, 52, 56)  \n",
      "17     datetime.datetime(2016, 11, 4, 23, 57, 37)  \n",
      "18     datetime.datetime(2016, 11, 4, 23, 50, 57)  \n",
      "19     datetime.datetime(2016, 11, 4, 23, 31, 28)  \n",
      "20     datetime.datetime(2016, 11, 4, 23, 14, 59)  \n",
      "21     datetime.datetime(2016, 11, 4, 20, 33, 12)  \n",
      "22      datetime.datetime(2016, 11, 4, 19, 7, 20)  \n",
      "23     datetime.datetime(2016, 11, 4, 18, 56, 15)  \n",
      "24     datetime.datetime(2016, 11, 4, 18, 46, 42)  \n",
      "25     datetime.datetime(2016, 11, 4, 16, 28, 36)  \n",
      "26     datetime.datetime(2016, 11, 4, 16, 26, 19)  \n",
      "27     datetime.datetime(2016, 11, 4, 16, 17, 22)  \n",
      "28     datetime.datetime(2016, 11, 4, 15, 32, 31)  \n",
      "29       datetime.datetime(2016, 11, 4, 2, 35, 8)  \n",
      "...                                           ...  \n",
      "18443   datetime.datetime(2017, 10, 4, 0, 25, 34)  \n",
      "18444    datetime.datetime(2017, 10, 4, 0, 24, 2)  \n",
      "18445   datetime.datetime(2017, 10, 4, 0, 20, 48)  \n",
      "18446   datetime.datetime(2017, 10, 3, 23, 2, 51)  \n",
      "18447  datetime.datetime(2017, 10, 3, 21, 59, 58)  \n",
      "18448  datetime.datetime(2017, 10, 3, 21, 22, 19)  \n",
      "18449  datetime.datetime(2017, 10, 3, 11, 40, 47)  \n",
      "18450  datetime.datetime(2017, 10, 2, 11, 11, 37)  \n",
      "18451   datetime.datetime(2017, 10, 2, 1, 22, 34)  \n",
      "18452   datetime.datetime(2017, 10, 1, 19, 1, 19)  \n",
      "18453  datetime.datetime(2017, 10, 1, 14, 31, 15)  \n",
      "18454  datetime.datetime(2017, 10, 1, 14, 30, 59)  \n",
      "18455  datetime.datetime(2017, 10, 1, 12, 30, 17)  \n",
      "18456  datetime.datetime(2017, 10, 1, 12, 26, 26)  \n",
      "18457  datetime.datetime(2017, 10, 1, 12, 22, 14)  \n",
      "18458        datetime.datetime(2017, 10, 1, 2, 8)  \n",
      "18459  datetime.datetime(2017, 9, 30, 23, 24, 18)  \n",
      "18460  datetime.datetime(2017, 9, 30, 22, 46, 47)  \n",
      "18461  datetime.datetime(2017, 9, 30, 22, 26, 55)  \n",
      "18462   datetime.datetime(2017, 9, 30, 22, 15, 2)  \n",
      "18463  datetime.datetime(2017, 9, 30, 20, 37, 10)  \n",
      "18464  datetime.datetime(2017, 9, 30, 19, 57, 29)  \n",
      "18465  datetime.datetime(2017, 9, 30, 19, 56, 46)  \n",
      "18466  datetime.datetime(2017, 9, 30, 19, 55, 20)  \n",
      "18467  datetime.datetime(2017, 9, 30, 19, 53, 51)  \n",
      "18468  datetime.datetime(2017, 9, 30, 19, 43, 54)  \n",
      "18469  datetime.datetime(2017, 9, 30, 19, 30, 17)  \n",
      "18470  datetime.datetime(2017, 9, 30, 19, 19, 36)  \n",
      "18471   datetime.datetime(2017, 9, 30, 18, 4, 59)  \n",
      "18472    datetime.datetime(2017, 9, 30, 12, 7, 9)  \n",
      "\n",
      "[18473 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "clean_df = clean_df_before.append(clean_df_after)\n",
    "clean_df = clean_df.reset_index()\n",
    "del clean_df['index']\n",
    "print(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chenge the filename and clean dataframes to write different data\n",
    "#df = pd.merge(clean_df_after, clean_df_before)\n",
    "#print(df.describe())\n",
    "\n",
    "with open('../data/clean_tweets/clean_tweets_all.json', 'w') as file:\n",
    "    file.write(clean_df.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "0     thank you iowa get out votetrumppence https https   \n",
      "1                rt https thank you new hampshire https   \n",
      "10    must act immediately clinton charity lawyer to...   \n",
      "100   join me tonight in cedar rapids iowa at pm htt...   \n",
      "1000  https new followers for https from to all in t...   \n",
      "\n",
      "                                       timestamp  \n",
      "0     datetime.datetime(2016, 11, 6, 19, 26, 58)  \n",
      "1      datetime.datetime(2016, 11, 6, 18, 9, 44)  \n",
      "10     datetime.datetime(2016, 11, 5, 21, 38, 1)  \n",
      "100   datetime.datetime(2016, 10, 28, 15, 39, 1)  \n",
      "1000    datetime.datetime(2016, 8, 20, 5, 2, 57)  \n"
     ]
    }
   ],
   "source": [
    "with open('../data/clean_tweets/clean_tweets_all.json', 'r') as file:\n",
    "    testi = pd.read_json(file)\n",
    "    \n",
    "print(testi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
