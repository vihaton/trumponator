{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../data/raw/Raw_tweets_DonaldTrump_before_elec.json'\n",
    "\n",
    "df = pd.read_json(open(dir, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datetime.datetime(2016, 11, 6, 19, 26, 58)</td>\n",
       "      <td>Thank you Iowa - Get out &amp;amp; #VoteTrumpPence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>datetime.datetime(2016, 11, 6, 18, 9, 44)</td>\n",
       "      <td>RT @IvankaTrump: Thank you New Hampshire! ????...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>datetime.datetime(2016, 11, 6, 17, 6, 8)</td>\n",
       "      <td>Van Jones: ‘There Is A Crack in the Blue Wall’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>datetime.datetime(2016, 11, 6, 6, 8, 32)</td>\n",
       "      <td>Great night in Denver, Colorado- thank you! To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datetime.datetime(2016, 11, 6, 4, 27)</td>\n",
       "      <td>RT @DanScavino: Join @realDonaldTrump LIVE in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   created_at  \\\n",
       "0  datetime.datetime(2016, 11, 6, 19, 26, 58)   \n",
       "1   datetime.datetime(2016, 11, 6, 18, 9, 44)   \n",
       "2    datetime.datetime(2016, 11, 6, 17, 6, 8)   \n",
       "3    datetime.datetime(2016, 11, 6, 6, 8, 32)   \n",
       "4       datetime.datetime(2016, 11, 6, 4, 27)   \n",
       "\n",
       "                                                text  \n",
       "0  Thank you Iowa - Get out &amp; #VoteTrumpPence...  \n",
       "1  RT @IvankaTrump: Thank you New Hampshire! ????...  \n",
       "2  Van Jones: ‘There Is A Crack in the Blue Wall’...  \n",
       "3  Great night in Denver, Colorado- thank you! To...  \n",
       "4  RT @DanScavino: Join @realDonaldTrump LIVE in ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      98\n",
       "1      70\n",
       "2      97\n",
       "3     138\n",
       "4     142\n",
       "5     135\n",
       "6      77\n",
       "7     140\n",
       "8      39\n",
       "9      83\n",
       "10    108\n",
       "11    112\n",
       "12    139\n",
       "13    133\n",
       "14    140\n",
       "15     25\n",
       "16    148\n",
       "17     80\n",
       "18    117\n",
       "19    138\n",
       "Name: pre_clean_len, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying out the lengths of the tweets:\n",
    "df['pre_clean_len'] = [len(t) for t in df.text]\n",
    "df['pre_clean_len'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#MakeAmericaGreatAgain #Trump2016 https://t.co/6hP6PzPkY5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying the cleaning with an example tweet first.\n",
    "df.text[3191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#MakeAmericaGreatAgain #Trump2016 https://t.co/6hP6PzPkY5\n"
     ]
    }
   ],
   "source": [
    "example1 = BeautifulSoup(df.text[3191], 'lxml')\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @IvankaTrump: Thank you New Hampshire! ???? https://t.co/pdnxXq2jgy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RT @IvankaTrump: Thank you New Hampshire! ???? https'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing links with \"https\"\n",
    "print(df.text[1])\n",
    "re.sub('https?://[A-Za-z0-9./]+','https',df.text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    Thank you Ohio! VOTE so we can replace Obamaca...\n",
       "21    Join me live in Wilmington, Ohio!\\nhttps://t.c...\n",
       "22    If Obama worked as hard on straightening out o...\n",
       "23    'ICE OFFICERS WARN HILLARY IMMIGRATION PLAN WI...\n",
       "24    Thank you NH! We will end illegal immigration,...\n",
       "25    Clinton Aides: ‘Definitely’ Not Releasing Some...\n",
       "26    RT @TeamTrump: Mrs. Saucier's son is in prison...\n",
       "27    \"The Clinton Campaign at Obama Justice\" #Drain...\n",
       "28    Join me today in Wilmington, Ohio at 4pm: http...\n",
       "29    There is no challenge too great, no dream outs...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Join my team over on my Facebook page- live now! #Debates\\nhttps://t.co/vpDVQfO58A https://t.co/5v1tWzHrxq'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = df.text[226]\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Join my team over on my Facebook page- live now! #Debates\\nhttps://t.co/vpDVQfO58A https://t.co/5v1tWzHrxq'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.replace(u\"\\ufffd\", \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' MakeAmericaGreatAgain  Trump     https   t co  hP PzPkY '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing special characters, leaving only letters.\n",
    "re.sub(\"[^a-zA-Z]\", \" \", df.text[3191])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual method for cleaning the text\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, 'https', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "\n",
    "    # Do we want to do something for links etc?\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", stripped)\n",
    "    lower_case = letters_only.lower()\n",
    "\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you iowa get out votetrumppence https https',\n",
       " 'rt https thank you new hampshire https',\n",
       " 'van jones there is a crack in the blue wall it has to do with trade https',\n",
       " 'great night in denver colorado thank you together we will make america great again icymi watch rally here https',\n",
       " 'rt https join https live in denver colorado via his facebook page we are here makeamericagreatagain https',\n",
       " 'thank you reno nevada nothing will stop us in our quest to make america safe and great again americafirst https',\n",
       " 'join me live in reno nevada https https',\n",
       " 'join me tomorrow minnesota pm https michigan pm https virginia p https',\n",
       " 'draintheswamp https',\n",
       " 'top clinton aides bemoan campaign all tactics no vision https']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing with small amount of texts\n",
    "testing = df.text[:10]\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "1000  tweets has been processed\n",
      "2000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/ZQ0osiFEJQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/SmTkLPiBYD\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/T5JBFXOz3F\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000  tweets has been processed\n",
      "6000  tweets has been processed\n",
      "7000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://t.co/PtViAyrO4A\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000  tweets has been processed\n",
      "9000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/h43dehf0WV\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/FXqSWusSTV\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/qZSEifBNaP\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/MgkotGmkJ0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/hIw1AQdRpY\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/LdXQb42Imc\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/RzqoiQ4SmD\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/zMg2iZgriM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/G0BjCXEnaX\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/Vu2b2hhwHu\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/54YVC4DDfe\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/Vh47XjGzpt\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/c79zLeREOA\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/H2FiSVxyOF\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/0a25gApyJ6\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/6ZG0P6FRs5\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/6v90Th0zl1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/3PAVDdfJJr\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/15ibBbf34U\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/wYCNmkkaNR\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/mJtO0AFLus\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/OGqKufBeHn\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/PeF12D2IqJ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/TfRmZA8RWQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/5kIR5EggBp\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/TmICRUV9uo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/Gm9KE8cHpS\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/E3xvdUGZqa\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/ue5JEZy85v\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/yfwdyUHmn3\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/tJG3KIn2q0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000  tweets has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/4OjDqTMEIx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/8lI2lomGkh\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/6VLQYAlcto\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/venla/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/UM3YJ6lUiD\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000  tweets has been processed\n",
      "13000  tweets has been processed\n"
     ]
    }
   ],
   "source": [
    "# Running the tweet_cleaner for the big data\n",
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(0, len(df)):\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print(i+1, \" tweets has been processed\")                                                                  \n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thank you iowa get out votetrumppence https https</td>\n",
       "      <td>datetime.datetime(2016, 11, 6, 19, 26, 58)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt https thank you new hampshire https</td>\n",
       "      <td>datetime.datetime(2016, 11, 6, 18, 9, 44)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>van jones there is a crack in the blue wall it...</td>\n",
       "      <td>datetime.datetime(2016, 11, 6, 17, 6, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great night in denver colorado thank you toget...</td>\n",
       "      <td>datetime.datetime(2016, 11, 6, 6, 8, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt https join https live in denver colorado vi...</td>\n",
       "      <td>datetime.datetime(2016, 11, 6, 4, 27)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  thank you iowa get out votetrumppence https https   \n",
       "1             rt https thank you new hampshire https   \n",
       "2  van jones there is a crack in the blue wall it...   \n",
       "3  great night in denver colorado thank you toget...   \n",
       "4  rt https join https live in denver colorado vi...   \n",
       "\n",
       "                                    timestamp  \n",
       "0  datetime.datetime(2016, 11, 6, 19, 26, 58)  \n",
       "1   datetime.datetime(2016, 11, 6, 18, 9, 44)  \n",
       "2    datetime.datetime(2016, 11, 6, 17, 6, 8)  \n",
       "3    datetime.datetime(2016, 11, 6, 6, 8, 32)  \n",
       "4       datetime.datetime(2016, 11, 6, 4, 27)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving clean texts into a dataframe with timestamps\n",
    "clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "clean_df['timestamp'] = df.created_at\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clean_tweets.json', 'w') as file:\n",
    "    file.write(df.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
